#!/usr/bin/env python
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, RetrievalQA
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
import os

import click as ck
import pandas as pd

# Model configuration
# base_url = os.environ.get("OLLAMA_API_BASE_URL", "http://127.0.0.1:11434")
# if base_url.endswith("/"):
#     base_url = base_url.rstrip("/")
#
#
# class Config:
#     MODEL = os.environ.get("MODEL", "llama3.1")
#     EMBEDDING_MODEL_NAME = os.environ.get("EMBEDDING_MODEL_NAME",
#                                           "all-MiniLM-L6-v2")
#     OLLAMA_API_BASE_URL = base_url
#     HUGGING_FACE_EMBEDDINGS_DEVICE_TYPE = os.environ.get(
#         "HUGGING_FACE_EMBEDDINGS_DEVICE_TYPE", "gpu")


@ck.command()
@ck.option(
    "--csv_file_path",
    "-c",
    type=ck.Path(exists=True),
    required=True,
    help="Path to the CSV file",
)
@ck.option("--question",
           "-q",
           type=ck.Path(exists=True),
           required=True,
           help="Question in the CSV file")
@ck.option("--output_folder_path",
           "-o",
           type=ck.Path(),
           required=True,
           help="Path to the output file")
@ck.option("--model",
           "-m",
           type=ck.Choice(["llama3.1", "llama3.2",'mistral',"phi3","phi3:14b","gemma2","gemma2:27b"]),
           default="llama3.1",
           required=False,
           show_default=True,
           help="Model")
@ck.option("--temp",
           "-t",
           type=float,
           default=0.8,
           required=False,
           show_default=True,
           help="Temperature")
@ck.option("--max_tokens",
           "-x",
           type=int,
           default=1000,
           required=False,
           show_default=True,
           help="Max tokens")
def csv_insights(csv_file_path, question, output_folder_path, model, temp,
                 max_tokens):
    """
    Query the CSV file and get insights

    """

    if not os.path.exists(csv_file_path):
        raise ValueError(f"CSV file not found: {csv_file_path}")

    if not os.path.exists(question):
        raise ValueError(f"Question not found: {question}")

    if not os.path.exists(output_folder_path):
        os.makedirs(output_folder_path)

    # Load the question CSV file
    try:
        q_df = pd.read_csv(question)
    except Exception as e:
        raise ValueError(f"Error reading CSV file: {e}")

    if q_df.empty:
        raise ValueError(f"Empty CSV file: {question}")
    if 'Question' not in q_df.columns:
        raise ValueError(f"Question column not found in CSV file: {question}")

    if 'QN' not in q_df.columns:
        raise ValueError(f"QN column not found in CSV file: {question}")

    # Load and process the CSV data
    loader = CSVLoader(csv_file_path)
    documents = loader.load()

    # Create embeddings
    embeddings = OllamaEmbeddings(model=model)

    chroma_db_path = csv_file_path.replace(
        ".csv", "")  # os.makedirs(vector_store_directory, exist_ok=True)
    os.system(f"rm -r {chroma_db_path}")
    # if not os.path.exists(chroma_db_path):

    chroma_db = Chroma.from_documents(documents,
                                          embeddings,
                                          persist_directory=chroma_db_path)
    # else:
    #     chroma_db = Chroma(persist_directory=chroma_db_path,
                           # embedding_function=embeddings)
    chroma_db.persist()

    llm = Ollama(model=model, temperature=temp, num_predict=max_tokens
                 )
    template = """
    ### System:
    You are an honest assistant.
    You will accept CSV files and you will answer the question asked by the user appropriately.
    If you don't know the answer, just say you don't know. Don't try to make up an answer.

    ### Context:
    {context}

    ### User:
    {question}

    ### Response:
    """

    prompt_template = PromptTemplate(
        input_variables=["context"],
        template=template,
    )

    # Set up the question-answering chain
    for _, row in q_df.iterrows():
        question = row["Question"]
        qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=chroma_db.as_retriever(),
            chain_type_kwargs={"prompt": prompt_template},
        )
        result = qa_chain({"query": question})
        print(result["result"])
        with open(f"{output_folder_path}/{row['QN']}.txt", "w") as f:
            f.write(result["result"].strip())
    # print(result["result"])


if __name__ == "__main__":
    csv_insights()

#
